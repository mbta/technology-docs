# Event Driven Architecture

- Feature Name: `event-driven-architecture`
- Start Date: 2023-03-21
- RFC PR: [mbta/technology-docs#0019](https://github.com/mbta/technology-docs/pull/19)
- Asana task: n/a
- Status: Accepted

# Summary

When events happen inside CTD applications, CloudEvents should be sent to a Kinesis Data Stream for other consumers to potentially use.

# Motivation

As CTD applications become widely used across the MBTA, the data generated by those applications can be useful olovelyutside the boundaries of that application. While we have had some integrations in the past, our previous approach does not nicely scale to the size of the current problem.

# Guide-level explanation

## When to use Event Driven Architecture?

This pattern is useful when applications need to communicate in real-time with other CTD applications. 

- real-time: events are handled as they happen
- CTD applications: applications built by vendors may not have native support for Kinesis. However, it can be possible to build an integration ourselves (as we do with Trike).

## When not to use Event Driven Architecture?

- If an application does not need to communicate with other CTD applications
- only receives or provides batch updates
- data exceeds the 1MB limit for a Kinesis record

## Events

Events are a message indicating that the state of the application has changed: they always happen in the past.

### Kinds of events

At a high-level, there are two kinds of events: notification and state-transfer. We prefer to use state-transfer events where possible, as they more effectively decouple the producers and consumers, and do not require producers to create an additional API integration.

**Notification events** Notification events indicate that something happened, but little additional information. For example, if there was an event for an alert being created, a notification event would only include the ID of the newly created alert. Consumers of these events need to use APIs or other integrations with the producer of the event to fetch any other required data, as it’s not present in the event.

**State-transfer events** State-transfer events include more of the additional information that event consumers use. Going back to our alerts example, a state-transfer alert would include all of the information about the alert, such as the description and the routes affected.

**Delta events** A subset of state-transfer events are called delta events, which represent changes to previously described entities. The delta event only includes the information which was changed (and potentially the previous values), rather than including the full set of information about the entity. A delta event for an alert being updated would only include the modified fields, and possibly the previous values which were modified, but not the entire contents of the alert.

## What events to generate?

Generating events allows consumers to be decoupled from the producers, while maintaining insight into what is happening within the producer. Producers SHOULD NOT expect that any particular consumer is listening to the events.

Producers SHOULD generate events for business-relevant actions taken either by users of the application or by automated processes.

Producers do not need to generate events for low-level events such as login/logout or individual UI actions. Rather, they should favor higher-level events representing the user’s intent. However, if lower-level events are also relevant to other consumers, producers MAY generate them as well.

Producers MAY use meetings such as [Event Storming](https://github.com/mariuszgil/awesome-eventstorming) or [Event Modeling](https://eventmodeling.org/) with their stakeholders to generate relevant business events. Even if it is not clear who the consumer of a particular business event may be, prefer generating more events rather than fewer.

# Reference-level explanation

## CloudEvents

[CloudEvents](https://cloudevents.io/) is “a specification for describing event data in a common way”. [RFC4](https://github.com/mbta/technology-docs/blob/main/rfcs/accepted/0004-socket-proxy-ocs-cloudevents.md) further discusses the motivations for using CloudEvents and their fields.

The contents of the `data` field SHOULD be self-describing. For example, the `com.mbta.glides.editors_changed` event is clear about what has happened, even without referring to the documentation. `com.mbta.ocs.raw_message` is an anti-example here: parsing the `raw` field requires additional knowledge outside the contents of the event. A future refactoring might be to include a parsed version of the message fields along with the raw message.

### Event types

The `type` field in the CloudEvent distinguishes the action which is being represented. Our event types generally live in the `com.mbta.ctd` namespace. Events from a particular application live under `com.mbta.ctd.<application>`. Event types are in the past tense and `snake_case`. For example, `com.mbta.ctd.glides.trips_updated`.

Events which do not map cleanly to an action, or come from a different team, SHOULD have names which reflect their contents. An example of this is Trike, which serves as a proxy for events generated by the OCS system, and generates events of the type `com.mbta.ocs.raw_message`.

## Kinesis Data Streams

[Kinesis Data Streams](https://aws.amazon.com/kinesis/data-streams/) is “a serverless streaming data service that makes it easy to capture, process, and store data streams at any scale.” It provides write-once, read-many semantics for our events, allowing multiple consumers to read events once they’ve been generated by the producer. [RFC5](https://github.com/mbta/technology-docs/blob/main/rfcs/accepted/0005-kinesis-proxy-json.md) further discusses the concepts of Kinesis Data Streams.

CloudEvents are encoded into JSON, and written to a Kinesis record. If present, the `partitionkey` field of the CloudEvent SHOULD be used as the `PartitionKey` for the Kinesis [PutRecord](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecord.html) API call.

Multiple CloudEvents MAY be batched together in a single PutRecord call, by wrapping them in a JSON array. Consumers of events MUST support receiving either a single CloudEvent or an array of CloudEvents in a single Kinesis record. Producers MUST NOT rely on a batch staying together throughout processing. If a producer needs information to be part of a single message, all the data should be wrapped in a single CloudEvent.

Kinesis guarantees 99.9% availability. If your application requires a higher SLA, consider including support for fallbacks such as DynamoDB which have higher SLAs. 

## Schemas

Schemas MUST be documented in the [mbta/schemas](https://github.com/mbta/schemas) repository with [JSON Schema](https://json-schema.org/) 2020-12 (or later) documents. Additional documentation SHOULD appear next to the schema with descriptions about what the events mean.

## Compatibility

As events may be stored indefinitely, the event schemas MUST remain compatible. This means that for a given event `type`, all events ever generated must match the latest schema version for that type. Additionally, the semantic meaning of an event MUST NOT change, even if the change otherwise is schema-compatible.

### Producer responsibilities

Producers are responsible for generating the event schemas, and for maintaining backwards and forwards compatibility.

If an event `type` needs to change in a backwards-incompatible fashion, a new `type` MUST be created. This can be done by adding a `.v2` suffix to the event, a `.YYYY-MM-DD` date suffix, or anything which creates a different `type`.

Producers MUST continue to generate both event types as long as there are consumers which do not understand the new type. For example, if an event has both a `.v1` and `.v2` version, the producer must send both events as long as some consumers only understand the `.v1` version.

### Consumer responsibilities

Consumers MUST ignore event `type`s that they do not understand. Consumers MUST ignore fields in events that they do not understand, trusting that the producer has followed their [responsibilities](about:blank#producer-responsibilities) to maintain compatibility.

## Personally-identifiable Information (PII)

Events which refer to users or operators may contain information about them: names, badge numbers, email addresses, or phone numbers (for example). If the event stream does contain PII, the Kinesis stream MUST be encrypted at rest. Consumers using the stream MUST take care that any PII they receive is properly stored: either ignored, or also encrypted. 

## Idempotent events

Due to the vagaries of the network, events may be sent to the network multiple times. Clients MUST be able to handle receiving an event multiple times. Producers SHOULD use the same  unique identifier (`id` in the CloudEvent) when re-sending an event to make it easier for clients to de-duplicate messages.

## Long-term Storage

As Kinesis only stores events for 24 hours by default, producers SHOULD record events into durable storage as well. [Kinesis Firehose](https://aws.amazon.com/kinesis/data-firehose/) is an AWS service which can perform this role, but is not the only approach.

Data can be written to an S3 bucket, preferably into a folder within the Data Platform infrastructure, and SHOULD be encrypted-at-rest (MUST if the events contain PII or potential PII). An alternative approach would be storing the events into an existing database, such as RDS, where the same encryption standards apply.

## Maintaining Application State

For consuming applications, there is a need to maintain an internal state based on the received events. This can take many forms depending on the application (S3 bucket, RDS, DynamoDB), but there are some particular pieces which need to be kept in order to ensure that each message is processed:

- the current shard IDs
- For each shard ID, the last sequence number processed
- If a batched message was partially processed, the index of the last processed part of the batch

These data ensure that if the consumer crashes, upon restarting the new consumer will pick up at exactly the same place.

# Drawbacks

Migrating to an event-driven architecture requires updates to applications which may be used to receiving a single file with the current state of the world. These applications will need to maintain their own state based on the received event messages.

Using Kinesis rather than Kafka means that events are only stored for 24 hours by default. This limits new consumers to only recent events, rather than allowing them to catch up on approximately all of the history easily.

Using JSON means that events are encoded to a larger size than they would in an alternative format.

# Rationale and alternatives

## GTFS-RT (Enhanced) Feeds

Historically, applications within CTD have used GTFS-RT feeds (or GTFS-RT Enhanced feeds with additional data) as their interchange format. 
- Busloc -> Concentrate 
- Busloc -> Skate 
- RTR -> Concentrate 
- RTR -> Real-time Signs 
- TrainLoc -> Concentrate

For the most part, this works well for applications which feed directly into Concentrate, which combines those GTFS-RT feeds into a single feed. There are some limitations to this approach.

- It’s tricky to handle non-public information (such as operator information from Busloc, or arrival times at skipped stops from RTR)
- Not all information we want to provide naturally fits into the GTFS-RT schema (such Waiver information from Busloc)
- A lot of the data is duplicated between updates

For data which is naturally GTFS-RT-shaped, and is being made public directly (or nearly directly), GTFS-RT feeds are still an alternative to consider.

## Alternative event distributions

Other message distribution options include: 
- queues, such as SQS or MQ 
- webhooks

Queues have the downside of only having a message read by a single consumer. This does not map to use the usecases envisioned here, which have multiple consumers.

Webhooks may be slightly more efficient, as messages are delivered directly from the producer to subscribed consumers. However, it requires more infrastructure on both sides. Producers need to maintain a list of consumers, and handle retrying messages which fail to send to a consumer. Consumers need to provide an HTTP endpoint to receive webhooks, along with opening an incoming firewall/security group port.

## Alternatives in other RFCs

Other alternatives called out in [RFC5](https://github.com/mbta/technology-docs/blob/main/rfcs/accepted/0005-kinesis-proxy-json.md): 
- Kafka, either with AWS Managed Kafka or running it ourselves 
- other encodings of CloudEvents, such as Avro or Protobuf

# Prior art

- [Trike](https://github.com/mbta/trike) uses this approach to provide messages to RTR for influencing predictions and to OCS Saver for future processing by LAMP / OPMI. This approach is described in [RFC4](https://github.com/mbta/technology-docs/blob/main/rfcs/accepted/0004-socket-proxy-ocs-cloudevents.md%5D) and [RFC5](https://github.com/mbta/technology-docs/blob/main/rfcs/accepted/0005-kinesis-proxy-json.md).
- [Glides](https://github.com/mbta/glides) uses this approach to provide events for inspector interventions into service. This approach is further described in [RFC18](https://github.com/mbta/technology-docs/pull/18).

# Unresolved questions

None at this time.

# Future possibilities

- At the moment, Elixir code for producing and consuming events is distributed between Glides, Trike and RTR. We could extract this out into a library which could be shared between applications, and include functionality such as verifying the schema on event read and/or write.
- Applications which could change to use this architecture:
    - Busloc could send events from TransitMaster such as vehicle locations and block waivers
    - KeyCloak could send events when users update their profile (e-mail, phone number, et cetera)
    - Applications which currently upload state-of-the-world feeds (RTR, Busloc, CommuterRailBoarding) could publish events when vehicle locations or predictions change, and Concentrate could consume them

